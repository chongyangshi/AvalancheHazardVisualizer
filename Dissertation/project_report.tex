\documentclass[openany]{UoYCSproject}

\protect\BEng
\protect\supervisor{Dr William A. P. Smith}
\protect\wordcount{11963}
\protect\includes{}
\protect\excludes{acknowledgements and bibliographies}
\protect\abstract{This project produces a functional web application to visualise potential avalanche hazards. Within a virtual 3D terrain reality, avalanche hazards along popular winter mountaineering routes can be observed. The project has a primary focus on Scottish mountains while providing excellent adaptability for other regions with appropriate data sources. Avalanche hazards were calculated from a combination of professional avalanche forecasts and spatial analysis of mountain terrain models. Both the accuracy and the usability of the application were evaluated through statistical comparisons with past avalanche records and user experimentation. Statistical evaluations conducted on our static risk model demonstrates better or comparable performance to past efforts. Additional features such as locating historical avalanches and route planning are also provided in the application. The safety and ethical considerations of the application being put into real world use are discussed.}
\protect\acknowledgements{I would like to thank Dr William Smith for his excellent guidance throughout the course of this project, making much of this possible.\\
I would also like to thank Dr Christopher Power and Dr Lilian Blot for their support throughout my academic study, as well as all three academics' references enabling my future academic studies.\\
Additionally, I would like to thank Rob Stone of the Department of Psychology for coming up with the original idea for this project, as well as for test-using the final application and providing valuable feedback towards its improvement.}

\usepackage[left=20mm,top=26mm,right=20mm,bottom=20mm]{geometry}    
\geometry{a4paper}
\usepackage{algorithm,algpseudocode}     
\algnewcommand\Or{\textbf{ or }}   
\algnewcommand\In{\textbf{ in }}   
\algnewcommand\NotIn{\textbf{ not in }} 
\makeatletter
\renewcommand{\ALG@beginalgorithmic}{\small}
\makeatother     		
\usepackage{graphicx}					
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{gensymb}
\usepackage{url}
\usepackage[parfill]{parskip}
\usepackage{natbib}
\usepackage{array}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{listings}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\setlength{\headsep}{7pt}
\graphicspath{ {images/} }
\setlength{\bibsep}{3pt plus 3ex}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\definecolor{notapplicable}{RGB}{192,192,192}
\definecolor{low}{RGB}{153,255,153}
\definecolor{moderate}{RGB}{255,255,153}
\definecolor{considerable}{RGB}{255,178,102}
\definecolor{high}{RGB}{255,102,102}
\definecolor{veryhigh}{RGB}{102,0,0}
\definecolor{notapplicable-new}{RGB}{255, 255, 255}
\definecolor{low-new}{RGB}{255, 255, 112}
\definecolor{moderate-new}{RGB}{230, 172, 0}
\definecolor{considerable-new}{RGB}{255, 127, 0}
\definecolor{high-new}{RGB}{255, 0, 0}
\definecolor{veryhigh-new}{RGB}{128, 0, 0}
\lstset{
  basicstyle=\ttfamily,
  columns=fixed,
  fontadjust=true,
  basewidth=0.5em
}
\setcounter{topnumber}{3}
\setcounter{bottomnumber}{1}
\setcounter{totalnumber}{3}

\title{An App for Visualisation of Avalanche Hazard}
\author{Chongyang Shi}
\date{May 1, 2017}							
\begin{document}

{\let\cleardoublepage\clearpage 
    \maketitle
}

\chapter{Introduction}

\section{Overview of the project}

The purpose of this project is to produce a functional web application to visualise potential avalanche hazards in Scottish Highlands. Through the application, the user is able to freely explore the virtual 3D terrain reality and observe avalanche hazards along popular mountaineering routes. Useful tools such as a route planner and a past avalanche locator are also included in the application.

A model for calculating localised avalanche hazards based on carefully sourced and processed avalanche forecasts \cite{sais} and terrain model data \cite{os-5} was produced. The accuracy of the model has been statistically evaluated against past avalanche records \cite[pp. 143-151]{scottish-avalanches}\cite{sais-map}. Usability and effectiveness of the application guiding the user away from hazardous locations were evaluated through test uses conducted with experienced mountaineers. 

The project incorporated tools and techniques from various aspects of Computer Science, including software engineering, geographic information system (GIS) modelling, human-computer interaction, computer graphics and algorithms. Many of these tools and techniques are associated with current research in these areas.

At the time of writing, the application can be accessed at \href{https://avalanche.ebornet.com/}{avalanche.ebornet.com}.

\section{Motivations behind the application} \label{sec:motivation}

As a front-runner of digital avalanche forecasts, the Scottish Avalanche Information Service (SAIS) \cite{sais} have been providing frequent and reliable winter avalanche forecasts for decades in Scotland.

In addition to written observations on conditions of the snowpack and weather, a typical SAIS avalanche forecast also includes a compass rose, providing avalanche risk levels on the scale of 1 to 5 for slopes of different aspects above and below a transition threshold altitude, shown in Figure \ref{fig:mapping}. However, while this compass rose provides a comprehensive overview on avalanche risks across the forecast region, it is not straightforward to interpret. The user will need to work out the surface aspects and altitudes along their route, and mentally infer the risk levels of different locations along their route. This is a tedious task, as it is not only easy to underestimate the risk due to risk levels varying sharply between neighbouring aspects and altitudes (as shown in Figure \ref{fig:mapping}), but also requires the user not to be unnecessarily cautious -- avoiding better routes with less-than-anticipated risk.

Therefore, the primary purpose of this project is to vastly simplify this process, by generating a coloured image based on the altitude and surface aspects at each point, which is then laid on top of a computer-generated terrain model in the 3D terrain viewer. Hence the information supplied by the compass rose can be visualised in the most straightforward way, allowing the user to freely navigate the 3D model and observe the hazard levels at each location along their route. The avalanche hazard of the same area mapped in Figure \ref{fig:mapping} is visualised as shown in Figure \ref{fig:stage1bennevis} (left).

In later stages of the project, we also seek to improve hazard representation through constructing a custom risk model based on information from the compass rose and terrain topographical analysis (Figure \ref{fig:stage1bennevis}, right); and to improve the functionalities of the application by implementing features such as pathfinding.

\begin{figure}[h]
		\centering
		\includegraphics[scale=0.9]{Mapping.png}
		\caption{\label{fig:mapping} An example compass rose produced by the SAIS on January 6, 2016 for Lochaber (left), showing a steep transition of risk levels; and an example manual inference of risk levels based on the compass rose and contour lines near Ben Nevis, Lochaber (right).  \cite{sais-lochaber0106}}
\end{figure}

\begin{figure}[h]
		\centering
		\includegraphics[height=4.5cm]{Stage1BenNevisRight.png}
		\includegraphics[height=4.5cm]{Stage3BenNevis.png}
		\caption{\label{fig:stage1bennevis} The avalanche hazard visualisation of the same area mapped in Figure \ref{fig:mapping}, generated in the 3D terrain viewer using the same avalanche forecast by both the essential framework (dynamic risk only, left) and the final product (both static topographical and dynamic risk with new colour coding, right).}
\end{figure}



\section{Stages of the project} \label{sec:stages}

In order to effectively track and manage the workload, the course of the project was divided into a requirement gathering stage and four development stages: the essential framework, static risk integration, risk-adjusted pathfinding, and evaluation \& testing. As the product from each stage is able to function independently of work done in later stages, unforeseen difficulties encountered during the research and development process can be effectively mitigated. The stages are described in Chapter \ref{ch:problem}.

\section{Considerations and statements on ethics}

The application was designed as a tool to improve winter mountaineering safety, therefore evidently the foremost consideration on ethics is regarding the accuracy and robustness of avalanche hazards projected by the application. If a location with imminent avalanche threat is reported as safe by the application, a user relying on the guidance of this application could be placed in serious danger. Conversely, if the application indicate avalanche warnings to users at a location where an avalanche is unlikely to occur, users may take unnecessary, time-consuming diversions that could expose the user to greater danger if daylight fades.

Due to the constraints placed on the lifecycle of this project, it is difficult to conduct an extensive peer review process to examine the accuracy of hazard projections. And while most open source libraries used during the development process are widely-used and have been well-tested, software errors may still occur and affect the safety assurance of the product. Furthermore, the lack of a known commercial or academic product with comparable functionalities eliminates the possibilities of cross-testing the software product.

As a result, while the application is functional and has shown promise during our own evaluations, it is not considered as a product ready for real-life use -- a warning of which is displayed when a user accesses the application.

\section{Structure of this report}

Chapter \ref{ch:lit-review} reviews relevant literature. Chapter \ref{ch:problem} describes the problem analysis and requirement gathering conducted in the planning stage. The theories studied and developed for the application, as well as for the processing of available data are discussed in Chapter \ref{ch:theory}. Chapter \ref{ch:app-description} describes in detail the architecture and features of the application, as well as its data processing procedures. This is followed by Chapter \ref{ch:app-testing}, which describes the evaluation and testing conducted on the application as well as its risk model. Finally, Chapter \ref{ch:conclusions} concludes the report, discusses its main contributions, and identifies potential future work.

\chapter{Literature Review} \label{ch:lit-review}

We first consider and evaluate past research and efforts made to understand and predict avalanches, as well as efforts in visualising these knowledge and predictions. While the term avalanche can be applied to various types of moving substances, such as rock and mud \cite[p. 1]{91097820150101}, only snow avalanches are studied by this project.

The literature review starts with an overview of the cause, classification and consequences of snow avalanches in Section \ref{sec:avalanche-background} to motivate the need of accurately visualising avalanche risks. This is followed by comparisons and critiques on existing methods and models for forecasting avalanches in Section \ref{sec:avalanche-forecasting}, which can be potentially applied in analysing avalanche forecasts and topographical data in Chapter \ref{ch:theory}. Past efforts in modelling and visualising avalanches risks are then discussed in Section \ref{sec:modelling-avalanche}, which provides reference theories in designing risk analysis models used in Chapter \ref{ch:theory}, and statistical evaluations in Chapter \ref{ch:app-testing}. Section \ref{sec:colour-representation} looks into the theories behind colour representations of risk, providing a basis for the colour design of the visualisation in Chapters \ref{ch:theory} and \ref{ch:app-description}. Section \ref{sec:terrain-pathfinding} discusses existing methods in cost calculations in 3D space, and compares researches in the pathfinding algorithm A* for algorithmic designs of the route planning tool in Chapter \ref{ch:theory}. Finally, Section \ref{sec:application-usability} covers existing Human-computer Interaction researches in 3D navigation and field usage, guiding the interaction design of the application.

\section{Background on snow avalanches} \label{sec:avalanche-background}

Accumulated snow is usually not a stable structure -- snow found in avalanches are usually 80\% air, and the snow cover gradually deforms as environment temperature approaches the melting point of water \cite[p. 16]{mcclung2006avalanche}. This instability fundamentally allows frequent occurrence of avalanches. 

Snow avalanches occur when large masses of snow or ice move rapidly down a mountainside or over a precipice, often triggered from the snow cover \cite[p. 1]{91097820150101}. Avalanches can either be triggered naturally due to combinations of certain geographical \cite[p. 17]{91097820150101} and meteorological \cite[p. 23]{91097820150101} conditions, or triggered by human activities nearby \cite{schweizer2001characteristics}. Often the latter is constructive to the former when an avalanche is triggered \cite[p. 17]{mcclung2006avalanche}\cite[p. 48]{scottish-avalanches}.

McClung and Schaerer \cite[p. 73]{mcclung2006avalanche} classified snow avalanches into two general types: loose snow avalanches, which usually involves surface snow and starts from a single area of the slope; and slab avalanches which are results of failures in the snow cover, featuring blocks of snow falling down the slope destructively. This classification is in agreement with most schemes, such as the avalanche classification by the UNESCO \cite{unesco-avalanche}, which also diversifies the classification by types of originating zones, paths and deposition zones. The diversified classification has been further improved by recent researches \cite{91097820150101}.

Regardless of type and triggering source, avalanches are often deadly to human presence nearby, especially when travelling outside areas with built-up defense \cite{91097820150101} (known as \textit{backcountry}), as is often the case for mountaineers. During the 45 years leading up to 1999, a total of 440 fatalities from avalanche incidents were recorded in the United States \cite{PAGE1999146}; while in the UK, avalanches in Scottish mountains alone claimed at least 73 lives during a similar time period \cite{scottish-avalanches}. 

With a prosperous winter sport industry bringing millions of people to mountain resorts every year \cite{hudson2003sport}, accurate forecasting of avalanches is becoming increasingly critical. 

\section{Avalanche forecasting} \label{sec:avalanche-forecasting}

Avalanche forecasting is defined as predictions of current and future instability of snow in space and time relative to a given triggering level for avalanche initiation \cite[p. 131]{McClung2002}. When applied, the main concerns of such predictions are risks to humans or property. 

A conventional approach to avalanche forecasting involves extensive field observations and testings of the snow cover by experienced forecasters and mountain guides, cross-referencing of historic meteorological and avalanche records, and utilisation of statistical and information theory methods on data collected to reduce decision uncertainties, as summarised by LaChapelle \cite{lachapelle1980fundamental} in 1980. This is still very similar to the approach the Scottish Avalanche Information Service (SAIS) use today \cite[p. 5]{sais2014-15}. In this style of approach, final decisions on hazard levels are heavily reliant on the experience and judgement of the experienced forecasters. LaChapelle noted \cite[p. 76]{lachapelle1980fundamental} that while inaccuracies do occur in forecasters' decisions, complete failures are rare. McClung \cite{mcclung2006avalanche} further analysed the influence of human perceptions in avalanche forecasting, and gave a formalised decision-making process to eliminate biases and improve accuracy.

However, human avalanche forecasting can sometimes be impractical or cost-prohibitive, due to area inaccessibility or vast size, as observed by Bühler \textit{et al.} \cite{buhler2013automated} in the northwestern Himalaya. McClung \cite{McClung1994} has explored as early as 1994 that computational methods can be used to improve data analysis and provide supplementary opinions through machine learning methods. Utilisation of computational methods also allow rapid coverage of new areas without undertaking increased human observations, as discussed by Lehning \cite{lehning1999snowpack}. Notable computational models on different types of data include \textit{SNOWPACK} and the NN method.

\textit{SNOWPACK}, a snow profile comparison method was first published by Lehning \textit{et al.} \cite{Lehning2001253} in 2001. The method attempts to establish a numerical profile of snow cover conditions based on various measured properties of the cover, such as the size and type of snow grains, temperature and density of the environments. An agreement score could then be established by comparing the numerical profile with an observed profile in the field. If high agreement scores are reached by the experiments, the numerical profile becomes suitable for use in avalanche forecasts. While SNOWPACK also considers dynamic environmental factors such as the influence of solar radiation and warm winds on the stability of snow layers \cite[p. 124]{bartelt2002physical}, it does not consider the influence of static terrain features on the stability of snow, such as the slope and the aspect.

However, attempts of applying SNOWPACK in other regions with different snow conditions have identified weaknesses in the assumptions made, which was originally developed for the Swiss Alps. A notable attempt was made by Hirashima \textit{et al.} \cite{Hirashima2008191} from 2005 to 2006 in Tsunan, Japan. While numerical profiles computed were in reasonable agreement with the observed snow profiles in Tsunan, it was found that the stability index estimators embedded in the model was not suitable for the shear strength of Japanese snow, and an alternative method for calculating the index had to be adopted. This suggested that adaptation of \textit{SNOWPACK} to a new region may require appropriate changes to the model based on local snow conditions.

While SNOWPACK was developed as a specialised method for analysis of snow covers, more generalised models such as the Nearest Neighbours (NN) method have also been developed. The NN model was initially developed by Buser \cite{buser1983avalanche} in 1983, which has since been supplemented by various studies, such as Purves \textit{et al.} \cite{Purves2003343} in 2003, and Singh and Ganju \cite{Singh2004105} \cite{Singh201533} since 2004. The NN model constructs a vector-space consisting of measurements on weather and terrain conditions on different days of observation, and applies a k-nearest-neighbour method to calculate the likelihood of an avalanche based on a new day's measurements. While measurements may be on different units and scales, which makes distance measurement difficult, this is partly compensated by applying scaling and weighting \cite[p. 599]{buser1983avalanche}. As the NN method has difficulties with high dimensional data, an alternative method based on Support Vector Machines (SVMs) have been proposed and developed \cite{Lehning2001253} \cite{pozdnoukhov2011spatio}. Another deficiency of the NN method is the increased uncertainty when there is not sufficient data available in the vector space \cite[p. 379]{pozdnoukhov2011spatio}.

\begin{figure}[h]
		\centering
		\includegraphics[scale=0.4]{ScotAvalanches1516.png}
		\caption{\label{fig:scotava1516} Reported avalanches in the Cairngorms and the Glencoe regions during winter sports season of 2015-2016, generated by the SAIS.\cite{sais-map}}
\end{figure}

The most notable application of the NN method is by the SAIS. Avalanche forecasts from the SAIS that are partially dependent on the NN method achieved a weighted accuracy of between 71\% and 82\% on visible winter days between 1998 and 2002 \cite[p. 351]{Purves2003343}. With up to 205 avalanches reported during the winter season of 2015-2016 \cite{sais} (as partly shown in Figure \ref{fig:scotava1516}), the SAIS plays a very significant role in saving lives from avalanches. Data from SAIS forecasts would also be a critical component in data sourcing and hazard modelling during this project, and is directly used to derive the hazard overlay in \textbf{Stage 1} of the project.

In addition to analysis of snow and terrain conditions and past avalanche records, studies of vegetations in mountainous areas can also provide important information on recurrence of avalanches. Christophe \textit{et al.} \cite{Christophe2010107} developed a model based on observation of tree-rings. As avalanches affect the growth of trees and other vegetations on the mountain slopes, growth condition of trees would provide useful insight into the magnitude and frequency of past avalanches in the area. One downside of the model is its unsuitableness for direct use in forecasting temporal avalanches, as the formation of general vegetation conditions require a long period of time. However, for the purpose of this project, this model could potentially be used for establishing a static risk factor for each point in the terrain, which is to be included in the overall risk model.

\section{Modelling and visualisation of avalanche hazard in GIS} \label{sec:modelling-avalanche}

In order to effectively store large amounts of geographical data for computing hazard map overlays, a geographic information system (GIS) \cite{clarke1986advances} is commonly used to organise avalanche hazard data.

While a majority of implemented avalanche forecasting methods -- such as those discussed in Section \ref{sec:avalanche-forecasting} -- provide generalised risk levels with respect to unique sets of environmental conditions, these generalised risk levels cannot be easily converted into localised hazards in a GIS, as these forecasting methods only operate on smaller subsets of parameters contributing to an avalanche. To evaluate the avalanche hazard of a specific location (e.g. Red Burn in Ben Nevis, Lochaber), a systematic method is often developed to calculate risk for each point in the region, taking into account as many hazard parameters as possible, which will be demonstrated by past researches discussed in this section.

According to Bühler \textit{et al.} \cite{buhler2013automated}, parameters influencing avalanche hazards can generally be placed in one of three categories: (a) terrain topography features at each point in the modelled area; (b) meteorological data of the general area; and (c) characteristics of the snow cover in the area.

Terrain topography features, such as aspect and slope \cite[pp. 17-18]{1456186}, convexity and concavity \cite[p. 267]{de2007geospatial}, and roughness \cite[p. 12]{wilson2007multiscale} can be computed directly from a terrain height-map raster. For the main area of study in this project -- Scotland, the government-owned Ordnance Survey has mapped the entire terrain of the British Isles (excluding Republic of Ireland) into a raster with resolution of 5 meters \cite{os-5-manual}. The data is updated frequently, and has a root-mean-square error of 2.5 meters in mountainous area. The level of resolution and precision is considered as capable of producing good representations of morphologic complexity and surface topography by Rayburg \textit{et al.} \cite{Rayburg2009261}. As terrain features outside urban areas are not subject to frequent change, once the aforementioned terrain features are computed, they do not need to be re-computed until the source terrain data updates.

Meteorological and snow profile data, on the other hand, are largely dependent on the current and recent weather conditions in the forecasting regions. Commonly examined parameters by past researches include temperature change, snow accumulation \cite{WARD1984109}, humidity \cite{durand1993meteorological}, and wind direction \cite{veitinger2014slab}; as well as snow profile parameters such as snow grain size and density (as analysed in the SNOWPACK model \cite{Lehning2001253}). However, as most past researches relied on weather forecasts in the regions in which they were conducted, not all parameters they used are available in Scotland. Fortunately, each SAIS avalanche forecast is also accompanied by a snow profile, containing measured meteorological and snow conditions observed during the field work of that forecast \cite{sais-snow-profiles}. 

A number of efforts have been made to produce GIS models of avalanche hazards based on some or all of the aforementioned hazard parameters.

\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{PreviousVisualisations.png}
		\caption{\label{fig:previous-visualisations} Hazard maps produced by prior efforts in visualising avalanche hazards: flow model and snow profile analysis by Cappabianca \textit{et al.} (top-left) \cite{Cappabianca2008193}; data-driven analysis by Pistocchi and Notarnicola (bottom-left) \cite{Pistocchi2013}; terrain topography analysis by Bühler \textit{et al.} (top-right) \cite{buhler2013automated}; and terrain topography and wind shelter analysis by Veitinger \textit{et al.} (bottom-right) \cite{veitinger2016potential}.}
\end{figure}

Grêt-Regamey and Straub \cite{Straub2006192} utilised a Bayesian network to study the relation between snow accumulation and flow model of avalanches, and visualised risks of avalanche impacts in an area with the trained network \cite{gret2006spatially}. A more comprehensive risk model was used by Cappabianca \textit{et al.} \cite{Cappabianca2008193} to produce a similar visualisation, as shown in Figure \ref{fig:previous-visualisations} (top-left). 

However, neither of the two studies above considered the full topography features of the triggering zones (e.g. lacking convexity and terrain roughness), some of which significantly affect the probability of release, as shown by Ghinoi and Chung \cite{Ghinoi2005305}. Bühler \textit{et al.} \cite{buhler2013automated} devised a considerably accurate release zone identification algorithm based on terrain topography features alone, as visualised in Figure \ref{fig:previous-visualisations} (top-right). 

More recent researches have also included meteorological analysis in the risk model. A notable example is by Veitinger \textit{et al.} \cite{veitinger2016potential} in 2016: in addition to topography features such as slope and roughness, wind sheltering effects of the terrain was computed based on the direction of wind, and included in the fuzzy logic computations of per-point risk. The algorithm's visualisation of avalanche release risk on a slope where a real avalanche took place is shown in Figure \ref{fig:previous-visualisations} (bottom-right). While evaluations showed that little accuracy improvement was achieved in the general case due to potential mapping errors, the algorithm was more accurate than its predecessor in locating frequent avalanche release areas.

Most hazard map visualisations of the aforementioned studies are in 2D only and are static once computed, providing no interactive capabilities. To our knowledge, only Pistocchi and Notarnicola \cite{Pistocchi2013} constructed a static 3D model with an avalanche risk overlay, as shown in Figure \ref{fig:previous-visualisations} (bottom-left). The risk model by Pistocchi and Notarnicola considers a mix of static terrain features (slope, aspect, curvature, etc.), land features (forest cover) and dynamic features (snow cover duration), and deploys both Weights of Evidence and Logistic Regression to compute the avalanche release potential of each area. At best cases, this model predicts about 70\% of avalanches in the 20\% of area classified as most hazardous. 

Risk visualisations produced by all past efforts discussed in this section are static imageries. For field uses, capabilities of displaying additional image layers and map features "on the fly", as well as the ability to observe from different angles and distances could be very helpful to users.

\section{Colour representation of risks} \label{sec:colour-representation}

While a realistic imagery of the surface can be produced in the 3D viewer by covering the artificial surface with corresponding aerial photographs, an appropriate colour representation scheme needs to be chosen for the hazard map overlay, which visualises the risk level at each point in the terrain model. Considerations need to be made regarding the choice of the colour scheme, as well as in what form the colours are displayed. While all colour schemes should have a unique representation for each distinct level of risk, how well an average user of the application perceives the colour scheme is a usability issue. The two main colour schemes proposed by existing researches and some attempts of applying them will be discussed in this section.

\subsection{Saturation-based colouring}

This is the simplest colour representation scheme. A single colour determined by one hue value is chosen, and in the context of visualising hazards, the colour of choice is often red \cite{aerts2003testing} \cite{cheong2016evaluating}. The saturation of this colour at different points in the visualisation would vary, dependening on the level of risk at that point. As summarised by Cheong \textit{et al.} \cite[Ch. 2]{cheong2016evaluating}, there have been conflicting conclusions from researches on the effectiveness of saturation-based risk visualisations, when compared with other schemes. Experiments also conducted by Cheong \textit{et al.} showed that the accuracy of user decisions under this scheme is good while multitasking, but otherwise not as good. 

While the subject of risk mapped by the visualisation of Cheong \textit{et al.} is wildfire hazard, a known visualisation of avalanche hazard under this saturation-based scheme was by Bühler \textit{et al.} \cite{buhler2013automated} as discussed in Section \ref{sec:modelling-avalanche}, although no human-computer interaction (HCI) evaluation was conducted. This scheme is also used by Spachinger \textit{et al.} \cite{spachinger2008flood} for flood risks.

\subsection{Hue-based colouring} \label{subsec:hue-based-colouring}

This colour representation scheme is similar to the saturation-based scheme, but uses different hues of colours (e.g. yellow and red) for different levels of risk in the visualisation. The European Avalanche Hazard Scale \cite[pp. 93-94]{91097820150101} -- also used by the SAIS in the UK -- applies such a hue-based scheme, as shown in Figure \ref{fig:eu-avalanche-scale}. This scheme is also used in visualisation of avalanche hazards by most studies discussed in Section \ref{sec:modelling-avalanche}, as shown in Figure \ref{fig:previous-visualisations}. 

\begin{figure}[h]
\small
\centering
\begin{tabular}{|>{\centering\arraybackslash}p{2.6cm}|>{\centering\arraybackslash}p{2.6cm}|>{\centering\arraybackslash}p{2.9cm}|>{\centering\arraybackslash}p{2.6cm}|>{\centering\arraybackslash}p{2.6cm}| } 
 \hline
 \cellcolor{low}Low Risk 
 & \cellcolor{moderate}Moderate Risk
 & \cellcolor{considerable}Considerable Risk
 & \cellcolor{high}High Risk
 & \cellcolor{veryhigh}\textcolor{white}{Very High Risk} \\
 \hline
\end{tabular}
\caption{\label{fig:eu-avalanche-scale} colour scheme reconstructed from the European Avalanche Hazard Scale, used by the SAIS \cite{sais-report}.}
\end{figure}
		
However, some studies have considered the hue-based scheme used in Figure \ref{fig:eu-avalanche-scale} to be insufficiently effective, most notably by Conger \cite{conger2004review}, who found that using green to represent low risk deviates from the common perception of green being a safety-indicating colour, which is not always the case in avalanche forecasting. Orange was also found to be sometimes inappropriate in association with a risk, and more difficult to print cartographically \cite[Ch. 4.2]{conger2004review}. Therefore Conger has suggested that the risk scale should be rescaled into four, in order to abolish the use of orange. However, other researches such as Braun and Silver \cite[p. 2212]{braun1995interaction} disagree with Conger's assessment of orange.

HCI evaluations conducted by Cheong \textit{et al.} \cite[Ch. 5]{cheong2016evaluating} concluded that while a hue-based scheme is often most preferred by users, its user decision accuracy is not the best among different colour representation schemes.
 
Other variations of applying hue-based colouring include an attempt by Gavaldà and Moner \cite{gavalda2008mountaineeroriented} to represent mountaineering route hazards with red-yellow-green traffic signals; as well as a method developed by Saito \textit{et al.} \cite{saito2005two}, using two tones of hue to represent hazard at each level, which in this case is unsuitable for the steep transitions of risk frequently occurring in avalanche hazard maps \cite[Ch. 5.1]{saito2005two}. Conger \cite[Ch. 5.2]{conger2004review} also suggested a method of partially filling hazard areas with triangles that are coloured according to the colour scheme. This however would be difficult to implement in practice, due to the size variations of neighbouring points with the same risk level.

\section{Cost-based terrain pathfinding} \label{sec:terrain-pathfinding}

Based on computations of per-point risk in \textbf{Stage 2} of the project, \textbf{Stage 3} focuses on implementing the extended functionality of terrain pathfinding, which will involve adapting existing pathfinding algorithms for use under the complicated conditions of backcountry terrain.

As according to Moore \textit{et al.} \cite[pp. 4-5]{moore1991digital}, the terrain height map raster from Ordnance Survey, as discussed in \ref{sec:modelling-avalanche} is essentially a square-grid network model of the real terrain. The network model was constructed from sampling the real elevation data at a resolution of 5 meters, which minimises the risk of missing abrupt changes in elevation. Pathfinding can then be conducted on the model. The most widely implemented algorithm for this purpose is the A* Search Algorithm \cite{4082128}, utilising heuristic analysis to guide a graph search based on Dijkstra's Algorithm \cite{Dijkstra:1959:NTP:2722880.2722945}. Most practical uses of search algorithms prefer the use of A* Search to a pure implementation of Dijkstra's Algorithm, as the former's heuristic function, when well-chosen, guides the search to a far quicker (although sometimes non-optimal) arrival at the goal, particularly in a large search space.

A* is most widely implemented in the development of pathfinding artificial intelligences in computer games \cite{cui2011based}. Since its introduction, a significant number of studies have been conducted to improve the performance of A* search, including iterative-deepening A* (IDA) \cite{korf1985depth}  which requires less memory at runtime; hierarchical path-finding A* (HPA) \cite{botea2004near}, which utilises hierarchical abstraction on large data grids to produce a near-optimal solution at very high performance; and \textit{navigational mesh} (NavMesh) \cite{tozour2002building}, which minimises the structure representation of data to reduce memory use and computation time. However, the hierarchical structure of HPA means that interconnection points between sub-level grids are often picked around obstacles, which do not exist on the elevation-based terrain grid, potentially hampering HPA's performance if applied.

When adapting A* or its aforementioned variants for computing a path in the terrain and taking into account the avalanche risk level at each point in the terrain raster, both the distance and the risk levels need to considered by the heuristic function of A*. Due to the nature of the uneven terrain, the proper walking distances between points can be estimated by Naismith's Rule and its variants \cite{magyari2012determining}. The accuracy of this estimation decreases as the resolution of the terrain raster decreases, which can occur if the terrain has to be down-sampled to reduce search space size.

To represent the points as a data structure, Yap \cite{yap2002grid} has suggested using a tiled hex grid to organise neighbouring points for optimisation of performance, to be analysed with a hierarchical search algorithm such as HPA \cite{botea2004near}. However, it is relatively difficult to re-project square grids of the terrain model into hex tiles, as terrain feature displacements can occur.

\section{Usability aspects of the application} \label{sec:application-usability}

As the project requires that the front-end application to be built is usable both on a mobile device with touchscreen and on a desktop computer with a mouse, best practises of design from existing HCI research on both types of user interfaces need to be considered, as well as the principles for a system that would potentially be used in the field rather than in office.

Nielson and Olsen \cite{nielson1987direct} proposed a formal method of manipulating the view of a 3D object (such as the virtual terrain globe in this application) with a 2D interface. This allows translations, rotations and scalings of the object to be performed by a single 2D event device, such as a mouse. In practise, methods of interactions have been mapped to specific combinations of actions of the mouse wheel and mouse keys \cite[pp. 195-197]{haklay2010interacting}. However, not all users find mouse interactions straightforward when navigating a 3D scene, and their satisfaction of the mouse interface largely depends on their proficiency with the mouse, as demonstrated by Dubois \textit{et al.} \cite{Dubois:2007:EAI:1531407.1531416}. 

On modern mobile devices, touchscreen is often the only interface available for the user to interact with the application. With mouse clicks replaced by touch interactions \cite[p. 191]{haklay2010interacting}, manipulations of the 3D object in the viewer are often less straightforward, as studied by Ku and Chen \cite{ku2014study}, which provided recommendations on designing touchscreen-based interactions with a 3D object. 

For an application that could be used in the field under time and user attention constraints, a study by Pascoe \textit{et al.} \cite{pascoe2000using} shows that the application should have context awareness and utilise interaction designs which minimise distraction to the user. Another study by Albinsson and Zhai \cite{Albinsson:2003:HPT:642611.642631} explored methods to improve the precision of touchscreen interactions under rough inputs from human fingers.
 
\chapter{Problem Analysis} \label{ch:problem}

This chapter covers the stage-by-stage (\ref{sec:stages}) plannings for the implementation of the project. This includes a summary of requirements for the application, and uses of existing software tools and libraries. Existing and new theories and methodologies required for each stage of the development will be discussed in detail in Chapter \ref{ch:theory}.

\section{Preliminary Stage: requirement gathering}

While the project specification has provided clear descriptions on the desired key functionalities, great flexibility is given to allow varying methods in achieving these functionalities, for which user opinions form a significant contributor. As the supervisor of the project is an experienced mountaineer, extensive consultations with the supervisor established the primary source of user opinions. Owing to the flexibility afforded, requirements are described in lieu of formal functional and non-functional specifications.

\textbf{Goal of the application}: to visually represent avalanche hazard levels on a map, with which the user will be able to navigate the terrain with minimised avalanche risk.

\textbf{Usability requirements}: The user must be able to use the application both on a desktop computer (mouse and keyboard) and on a mobile device (touchscreen). Due to the storage limitations of contemporary mobile devices, Internet accessibility is assumed to be available for live transmission of data. The performance requirement of the application shall be lenient enough to be usable on most modern smartphones and computers.

\textbf{Interaction requirements}: For the essential framework (\textbf{Stage 1}), the user shall be able to freely navigate a virtual 2D or 3D scene, on which avalanche risk levels are overlaid on top of an accurate terrain map. The colours used for the avalanche risk levels should follow the scheme used by the SAIS. For the static risk integration in \textbf{Stage 2}, a new colour coding for risk levels should be developed, and it shall be more distinctive and representative of the risk levels implied. A button should also be added to allow the user to switch the application back to \textbf{Stage 1}'s dynamic risk-only overlay.

In \textbf{Stage 3}, a new tool should be added to the user interface. Through the tool, the user shall be able to specify a start point and an end point on the terrain for pathfinding. A slider should be made available so that the user can configure the weight of distance and risk when considering a path. The user should also be able to clear the existing paths plotted on the terrain with another button. In addition, this stage shall also add a past avalanche locator tool, which will plot on the terrain one red dot for each historic avalanche recorded by the SAIS. When clicking on the dot the user shall see the time and description of that avalanche. The user should be able to switch the past avalanche time frame, such as in the past month or year.

\textbf{System requirements}: In order to generate the visualisation to the user, the system must be able to process the data required. In \textbf{Stage 1}, the system should be able to retrieve and store the data from the SAIS through web crawling, and pick the correct data for each pixel on the map by reading the height and aspect value from raster files. For the static risk integration in \textbf{Stage 2}, the system should be able to compute a static risk value based on various terrain properties for each pixel on the map. The static risk value should be converted by the system for suitable visualisation in the new colour coding. 

In \textbf{Stage 3}, the system should be able to compute a path between two points on the terrain specified by the user, based on a specified weighting between minimising path and risk. The system should return a set of coordinates representing the path to be plotted. The pathfinding operation should take a short amount of time for any reasonable input by the user.

\textbf{Development requirements}: The project should be developed in a version control system such as Git, and a detailed deployment documentation should be provided to allow any experienced system administrator to redeploy the application. The code written should be well-documented for future work.

\section{Stage 1: the essential framework}

While the interaction requirements do not specify whether the risk map should be a 2D or 3D scene, it is decided in the first stage of development that a 3D terrain will be produced by the application, allowing the user the maximum flexibility in navigating and observing the model.

In order to implement the essential framework required for \textbf{Stage 1}, it is necessary to first choose the programming languages and associated libraries for the functionalities.

Due to the requirement of making the application available to both desktop and mobile devices, the easiest way to develop for both platforms would be to produce a web application, taking advantage of the \href{https://get.webgl.org/}{WebGL} library to render the 3D terrain in web browsers. The Cesium \cite{cesium} framework was chosen to for the front-end of the application, as it already supports almost all the functionalities desired by the requirements, as well as excellent extensibility to implement the rest. The framework uses HTML5, CSS3 and JavaScript. The user interface will be described in \ref{sec:user-interface}.

For the back-end of the application, Python was chosen as the programming language used. In addition to the author's existing expertise with the language, Python also provides a wide range of libraries for interfacing with the required components. The dynamically-typed syntax also allows rapid prototyping and testing of the application. 

The main Python libraries chosen for the operational components of the application are: \href{http://www.seleniumhq.org/}{Selenium} for SAIS data retrieval, \href{http://flask.pocoo.org/}{Flask} for building the API server, \href{https://python-pillow.org/}{Pillow} for imaging the overlay colours, and \href{http://www.numpy.org/}{NumPy} for associated numerical computations.

With languages and libraries chosen, the implementation of the essential framework will be relatively straightforward. The back-end will simply read the dynamic risk data from the SAIS (Sections \ref{sec:dynamic-factors} and \ref{subsec:dynamic-data-processing}) based on height and aspect values of each point, generate overlay images and return them to the front-end. The full operation of the system is described in \ref{sec:app-system}.

\section{Stage 2: the static risk integration}

As \textbf{Stage 2} focuses on improving the data model and colour coding, no architectural change will be required at this stage. 

To take into account of static risk factors, additional computations will need to be conducted, the results of which will supplement the image generation process developed in the previous stage. Some techniques used in this stage will be adapted from existing techniques surveyed in Sections \ref{sec:avalanche-forecasting} and \ref{sec:modelling-avalanche}, while others will need to be developed anew, to be discussed in \ref{sec:static-factors}.

Minimal changes will need to be made to allow the application to read more data, and compute them differently. The combination of dynamic and static risk models will be discussed in \ref{sec:combine-models}. Changes to the risk colour coding at this stage will be covered in \ref{sec:chosen-colour-coding}.

\section{Stage 3: pathfinding and past avalanches}

The main addition to the application in \textbf{Stage 3} is the risk-adjusted pathfinding tool. Another tool for showing the locations of past avalanches is also added in this stage to assist the user in assessing the route provided. 

The user interface changes will involve the addition of pathfinding panel according to the requirements, as well as the ability to display the locations and information of past avalanches.

The back-end changes will be more complex. The new pathfinding functionality will require extensive modifications to the standard A* search algorithm (as described in \ref{sec:terrain-pathfinding}) in order to satisfy unique requirements posed by the application, which will be covered in \ref{sec:risky-pathfinding}. This also results in a data API for the API server, to be covered in \ref{subsec:data-api}.

For displaying past avalanches, a new data crawler for past avalanche data will be required (to be discussed in \ref{subsec:past-avalanche-data}), as well as the addition of another data API (to be covered in \ref{subsec:past-avalanche-data}).

\section{Stage 4: evaluations}

The final stage of the project involves evaluations on two aspects of the application: the usability of the application and the accuracy of the devised risk model. 

The usability evaluation (Section \ref{sec:usability-eval}) will involve user-testing of the application with another experienced mountaineer. During the user-testing, the user will be asked to try functionalities developed across different stages of the project in a simulated scenario, and inquired about their opinions on the user interface, as well as their confidence on the application assisting them to avoid avalanche hazards.

The evaluation of the risk model will be covered in Sections \ref{sec:stats-eval} and \ref{sec:risk-model-eval}.

\chapter{Theories and Methodologies} \label{ch:theory}

This chapter provides the details of the theories in avalanche forecasting, spatial analysis, risk representation and search algorithms applied across different stages of the project. Section \ref{sec:dynamic-factors} will be utilised in the construction of essential framework in \textbf{Stage 1}, which is built-on in \textbf{Stage 2} with Sections \ref{sec:static-factors} and \ref{sec:chosen-colour-coding}. Section \ref{sec:risky-pathfinding} provides the basis of algorithm design and implementation in \textbf{Stage 3}. Finally, Section \ref{sec:stats-eval} provides theoretical basis of statistically evaluating the devised risk model against past efforts in \textbf{Stage 4}.

\section{Dynamic risk factors of avalanche release} \label{sec:dynamic-factors}

Dynamic factors in avalanche forecasting are considered as factors that change over time, including but not limited to snow precipitation, conditions of snow cover (both well-studied by SNOWPACK \cite{Lehning2001253} as described in Section \ref{sec:avalanche-forecasting}), air temperature, wind direction and windspeed (studied by Veitinger \textit{et al.} \cite{veitinger2016potential} as described in Section \ref{sec:modelling-avalanche}). These factors also form the major elements of the Nearest Neighbour (NN) forecasting method (\ref{sec:avalanche-forecasting}), which is used by the project's source of dynamic risk data -- the Scottish Avalanche Information Service (SAIS). 

Purves \textit{et al.} \cite{Purves2003343} described the process utilised by the SAIS to forecast avalanche risk in Scottish mountains. Through an NN modelling software tracking observed past avalanches, forecasters will input measurements on temperature, wind and snow conditions, which are then scaled to the same range, and weighted by an autonomous genetic algorithm in order to avoid overfitting on recent avalanche conditions. This process creates a vector in the NN vector-space. The software then provides the input vector's nearest neighbours, the conditions of which then allows experienced forecasters to make a decision on the level of risk for each segment of aspect and altitude in each region. The resulting compass rose-style diagram can be seen in Figure \ref{fig:mapping}.

Based on the existing evaluations of the accuracy of SAIS avalanche forecasts \cite[p. 351]{Purves2003343} \cite{heierli2004verification}, with an accuracy of above 70\% on clear winter days, we have confidence that the dynamic risk model used by the SAIS provides a relatively reliable overview of dynamic avalanche risks across different aspects and altitudes, through a composition of different dynamic factors available to the SAIS observers. Therefore, SAIS risk data were applied directly to the terrain model for the essential framework developed in \textbf{Stage 1}.

\section{Static risk factors of avalanche release} \label{sec:static-factors} % Surface fitting.

Static factors in avalanche forecasting mainly concern the terrain topographic features that rarely change over time. Based on a height map of the terrain, various features can be computed through mathematical formulations. As summarised in Section \ref{sec:modelling-avalanche}, past efforts in modelling avalanche risks based on terrain features have generally covered the following features: altitude, slope, terrain curvature (convexities and concavities) and aspect. While past researches such as Ghinoi and Chung \cite[p. 313]{Ghinoi2005305} have attempted to reclassify altitudes data into distinct altitude bands to take into account the effects of altitude on avalanche risks, this has been equivalently considered in the dynamic risk model (\ref{sec:dynamic-factors}). Therefore, only slope, curvature and aspect are considered in the static risk model of this project. 

\subsection{Surface fitting with a polynomial}

By definition, slope, curvature and aspect are topography properties relative to a point's neighbourhood, usually within a window of 8 surrounding points in a rectangular grid. Therefore, to calculate these properties for each point, it is necessary to fit a polynomial surface within each such neighbourhood. A commonly used process is to fit a second degree polynomial surface to the neighbourhood. While it is generally agreed that a 2nd order polynomial is the most suitable for the 9-point 3D surface \cite{zevenbergen1987quantitative} \cite{heerdegen1982quantifying} \cite{evans1979integrated}, past efforts have applied different configurations of parameters. The configuration chosen for this project is a 9-parameter polynomial by Zevenbergen and Thorne \cite[p. 49]{heerdegen1982quantifying}, which has an order of 2 in each of the x and y directions, as shown in \ref{eq:surface-polynomial}. This model has the advantage of exactly passing through each point in the neighbourhood with uniquely determined parameters \cite[Sec. 6.1.3]{de2007geospatial}.

\begin{equation} \label{eq:surface-polynomial}
\fontsize{10}{11}\selectfont
Z = Ax^{2}y^{2} + Bx^{2}y + Cxy^{2}+Dx^{2} + Ey^{2} + Fxy + Gx + Hy + I
\end{equation}

Where coefficients \textit{A} to \textit{I} are determined by solving a linear system of equations between coordinates of (\textit{x}, \textit{y}) and known \textit{Z} values in the height map. The point where the neighbourhood is centered on is (0, 0), and the grid is expressed as followed:

\begin{center}
\fontsize{10}{11}\selectfont
\begin{tabular}{ccc}
(-1,1) & (0,1) & (1,1) \\
(-1,0) & (0,0) & (1,0) \\
(-1,-1) & (0,-1) & (1,-1) \\
\end{tabular}
\end{center}

As the center pixel (0, 0) is the point where we would like to determine the values of slope, curvature and aspect, which are all derived from the first or second derivatives of the surface polynomial \cite[p. 312]{Ghinoi2005305}. Only constant terms in the derivatives will be required for the calculations, as explained in \ref{subsec:slope}, \ref{subsec:curvature} and \ref{subsec:roughness}.

\subsection{Modelling avalanche risk from slope} \label{subsec:slope}

The slope angle represents the steepness of the terrain at a point, which can be determined by calculating the magnitude of change (first derivative) in both directions \cite[p. 50]{zevenbergen1987quantitative}. Practically, negation is applied to the value to represent that the slope is down-hill, but this is not required in modelling avalanche risk.

The first order partial derivatives of the surface polynomial described in \ref{eq:surface-polynomial} are as followed:

\begin{equation} \label{eq:first-derivatives}
\begin{aligned}
\fontsize{10}{11}\selectfont
\frac{\partial{Z}}{\partial{x}} = 2Axy^{2} + 2Bxy + Cy^{2} + 2Dx + Fy + G \\
\frac{\partial{Z}}{\partial{y}} = 2Ax^{2}y + Bx^{2} + 2Cxy + 2Ey + Fx + H
\end{aligned}
\end{equation}

At the center point where the slope angle is required, with coordinates (0, 0), it is trivial to determine the magnitude of change:
\begin{equation} \label{eq:first-derivatives-zeros}
\begin{aligned}
\fontsize{10}{11}\selectfont
&\textrm{When} \   x = 0,\ y = 0: \\
&\frac{\partial{Z}}{\partial{x}} =  G, \   \frac{\partial{Z}}{\partial{y}} = H \\
&\Rightarrow |SLOPE| = \sqrt{G^{2} + H^{2}}
\end{aligned}
\end{equation}

According to Horn \cite{1456186}, the slope angle is simply the inverse tangent of the magnitude in degree form: 
\begin{equation}
\fontsize{10}{11}\selectfont
S_t = \tan ^{ - 1} (|SLOPE|) = \tan ^{ - 1}(\sqrt{G^{2} + H^{2}})
\end{equation}

The consensus of most researches is that the majority of avalanches occur between a slope angle range of 25\degree \ and 60\degree, with the highest level of risk near the middle of the range \cite[p. 42]{91097820150101}\cite[p. 112]{mcclung2006avalanche}. Veitinger and Sovilla \cite[p. 2215]{veitinger2016potential} modelled this observation as a bell-shaped curve, which has been adapted by our model. Our model made two modifications to the curve. The first was moving the symmetrical line in x direction to \textit{x} = 42.5\degree \ to make the curve fit better with the McClung \cite[p. 112]{mcclung2006avalanche} model, as shown in Figure \ref{fig:slope-curve}.

\begin{figure}[h]
		\centering
		\includegraphics[height=6.25cm]{slope-curve.png}
		\begin{equation} \label{eq:slope-curve}
		\fontsize{10}{11}\selectfont
		P(avalanche | S_s) = \frac{1}{(1+(\frac{S_t-42.5}{8})^{6})} \cdot \frac{1}{C_s}
		\end{equation}
		\caption{\label{fig:slope-curve} Adapted bell-shaped curve for modelling avalanche risk of slope angles, normalised to a probability density function (PDF).}
\end{figure}

The second modification is that the curve can now be normalised to have an integral of 1 over its valid range by dividing with its integral $C_s$ between $[0, 90]$ (valid slope angle degrees), thus becoming a probability density function. This will be followed when modelling the other two static risk factors: curvature (Section \ref{subsec:curvature}) and terrain roughness (Section \ref{subsec:roughness}). 

\subsection{Modelling avalanche risk from curvature} \label{subsec:curvature}

Another significant static influence on the risk of avalanche release is terrain curvature, which can either be convex, concave or linear, as shown in  \ref{fig:convex-concave}. Along the vertical and the horizontal plane, there are two different types of terrain curvatures: profile curvature and planform curvature \cite[p. 50]{zevenbergen1987quantitative}. As the release of avalanches are mainly concerned with gravity in vertical plane, only the profile curvature is considered in our risk model.

\begin{figure}[h]
		\centering
		\includegraphics[height=3cm]{convex-concave.png}
		\caption{\label{fig:convex-concave} An illustration of concave (A), convex (B) and linear (C) terrains. \cite[p. 360]{menno2013map}}
\end{figure}

In conformance with the convention, our model defines the value of the profile curvature as one over the radius of the neighbourhood curvature in meters. As with illustrated in Figure \ref{fig:convex-concave}, concave curvatures are expressed as negative, convex curvatures are expressed as positive, while linear terrain is defined to have a curvature value of zero. 

As observed by McClung \cite[p. 115]{mcclung2006avalanche}, a majority of avalanches occur on terrains that are without significant curvature (forming a region of similar risk levels near a curvature value of zero), and convex terrains tend to increase the probability of slab avalanches by gliding. Conversely, snowpacks taper off on concave slopes, which reduces the probability of slab avalanches. Therefore, we produced a new probability density function for our model in modelling avalanche risk from curvature $Cur$, as shown in Figure \ref{fig:curvature-curve}.

\begin{figure}[h]
		\centering
		\includegraphics[height=6.25cm]{curvature-curve.png}
\begin{equation} \label{eq:curvature-equation}
\fontsize{10}{11}\selectfont
Cur = \frac{-2(DG^2+EH^2+FGH)}{G^2+H^2}
\end{equation}
\vspace*{-0.75cm}
\begin{equation} \label{eq:curvature-curve}
\fontsize{10}{11}\selectfont
  S_c =
  \begin{cases}
    Cur^3 + 0.5 & -0.7937 \leq Cur \leq 0.7937 \\
    1 & Cur > 0.7937 \\
    0 & \text{otherwise}
  \end{cases}
\end{equation}
\vspace*{-0.75cm}
\begin{equation}
  P(avalanche | S_c) = S_c \cdot \frac{1}{C_c}
\end{equation}
\caption{\label{fig:curvature-curve} A spline for modelling avalanche risk of terrain curvature, normalised to PDF with its integral $C_c$ between $[-0.7937, 0.7937]$ (uncapped range of terrain curvature).}
\end{figure}

As shown by (\ref{eq:curvature-curve}), the numerical model of the spline is based on the curve of function $y = Cur^3 + 0.5$, with a limited valid range. Extreme curvatures represented by $Cur > 0.7937$ or $Cur < -0.7937$ (curvature radius $r < 1.26m$) will be capped at extreme risk probabilities $P = 1$ and $P = 0$ respectively. We believe that the shape of the spline models the aforementioned effects of changing terrain curvature well: as the terrain levels off from a concave, the avalanche risk increases steeply until approaching the linear region, where there is no significance change in the risk level; and as the terrain curvature moves away from this region into convex, the avalanche risk increases again steeply. The curvature value $Cur$ can be easily computed by taking the second derivative of (\ref{eq:surface-polynomial}) with respect to the aspect angle (calculated by $\theta = \textrm{atan}(\frac{H}{G})$) in the fitted neighbourhood \cite[Ch. 6.2.2]{de2007geospatial}.

\subsection{Modelling avalanche risk from terrain roughness based on aspect and slope} \label{subsec:roughness}

While slope angle and curvature have a direct influence on the risk of avalanche release \cite[p. 1311]{buhler2013automated}, aspect angle does not hold a significance in avalanche risk itself, as it only indicates the direction in horizontal plane the terrain is facing. Instead, aspect angle is used in conjunction with slope angle to calculate the roughness of the terrain, which measures the mechanical resistance of the terrain on the snow cover. The higher the mechanical resistance, the harder it is for a large scale avalanche to be released \cite[p. 313]{Ghinoi2005305}.

There are various methods for calculating a terrain roughness index, some of which involves the consideration of landcover, which may change between seasons. Other factors such as changes in vegetation due to planting and logging may also affect the accuracy of a roughness index calculated from landcover data. Therefore, an alternative method based on elevation can be used. A notable method is based on vector dispersion, originally developed by Sappington \textit{et al.} \cite{sappington2007quantifying} and improved by Veitinger and Sovilla \cite[p. 2216]{veitinger2016potential}.

The method works by decomposing the fitted neighbourhood of nine points into three components ($X$, $Y$ and $Z$) based on the values of slope angle ($\alpha$) and aspect ($\beta$) at each point. Then for each of the three components, we sum up the neighbourhood. A resulting vector $|r|$ can then be calculated for the center point by further summing up the squares of the three neighbourhood sums and taking the square root. Finally, the roughness index $R$ can be determined by normalising the vector with the number of points in the neighbourhood (that is, 9) and subtract it from 1. The process is shown as followed:

\begin{equation}\label{eq:roughness-equation}
\begin{aligned}
\fontsize{10}{11}\selectfont
d_{xy} &= 1 \cdot \sin(\alpha) \\ &\Rightarrow Z = 1 \cdot \cos(\alpha),\  X = d_{xy} \cdot \cos(\beta),\  Y = d_{xy} \cdot \sin(\beta) \\
|r| &= \sqrt{(\sum X)^2 + (\sum Y) ^ 2 + (\sum Z) ^ 2} \\
R &= 1 - \frac{|r|}{9}
\end{aligned}
\end{equation}

From (\ref{eq:roughness-equation}), it is easily observed that the value of $R$ is always positive. The tested model by Veitinger and Sovilla \cite[p. 2216]{veitinger2016potential} believes that avalanche risk decreases sharply between 0 and 0.01, and further decreases slowly between 0.01 and 0.02. At a roughness level of above 0.02, an avalanche release will be unlikely. Therefore, a similar bell shaped curve of a probability density function is used as shown in Figure \ref{fig:roughness-curve}, albeit only using the relevant section where $R > 0$.

\begin{figure}[h]
		\centering
		\includegraphics[height=6.25cm]{roughness-curve.png}
\begin{equation} \label{eq:roughness-curve}
\fontsize{10}{11}\selectfont
P(avalanche | S_r) = \frac{1}{(1+(\frac{R+0.005}{0.01})^{4})} \cdot \frac{1}{C_r}
\end{equation}
\caption{\label{fig:roughness-curve} A bell-shaped curve for modelling avalanche risk of terrain roughness index, normalised to PDF with integral $C_r$ between $[0, \infty]$ (valid roughness index values).}
\end{figure}

\subsection{Combining static risk models} \label{subsec:combine-static}

With three different static risk factors calculated: slope angle, curvature and roughness, a method of uniforming the three into a single per-point risk value is needed. While it is possible to apply a Support Vector Machine (SVM) \cite{pozdnoukhov2011spatio} as described in Section \ref{sec:avalanche-forecasting} or a fuzzy logic operator \cite[p. 2218]{veitinger2016potential}, because all three static risk factors are modelled to be probability density functions (PDFs), the most straightforward method of combining them is to normalise each curve and multiply the resulting probabilities at each point. This also has the advantage of avoiding potential biases caused by the choice of weighting factors in a SVM or fuzzy operator. Value of each risk factor at each point has been divided by its normalising constant as described earlier, and then multiplied together at each point, shown as followed:

\begin{equation}\label{eq:combining-static-risk}
\fontsize{10}{11}\selectfont
P(avalanche | S_s, S_c, S_r) = P(avalanche | S_s) \cdot P(avalanche | S_c) \cdot P(avalanche | S_r) 
\end{equation}

\section{Colour visualisation of risk} \label{sec:chosen-colour-coding} 

For the essential framework completed in \textbf{Stage 1}, a colour scheme identical to the colour coding used by the SAIS (as shown in Figure \ref{fig:eu-avalanche-scale}) was used for risk representation (which at \textbf{Stage 1} relies only on the dynamic risk model). However, as described in Section \ref{sec:colour-representation}, past researches have found this colour coding scheme to be ineffective and has the potential to mislead the user. Therefore, a new colour scheme was implemented, as shown in Figure \ref{fig:new-avalanche-scale}.

\begin{figure}[h]
\small
\centering
\begin{tabular}{|>{\centering\arraybackslash}p{2.6cm}|>{\centering\arraybackslash}p{2.6cm}|>{\centering\arraybackslash}p{2.9cm}|>{\centering\arraybackslash}p{2.6cm}|>{\centering\arraybackslash}p{2.6cm}| } 
 \hline
 \cellcolor{low-new}Low Risk
 & \cellcolor{moderate-new}Moderate Risk
 & \cellcolor{considerable-new}Considerable Risk
 & \cellcolor{high-new}\textcolor{white}{High Risk}
 & \cellcolor{veryhigh-new}\textcolor{white}{Very High Risk} \\
 \hline
\end{tabular}
\caption{\label{fig:new-avalanche-scale} The new colour scheme implemented in \textbf{Stage 2}.}
\end{figure}

Recommendations made by Conger \cite{conger2004review} was partially applied: green has been eliminated from the colour scheme to avoid giving the user a false perception of safety; however, the recommendation to eliminate orange was not applied, due to inconsistencies in findings on orange between past researches, and orange's wide usage in risk colour codings. 

With the elimination of green, yellow is instead used for representing the low risk level, while a new risk colour with a hue value between yellow and orange, but lower lightness than both in the HSL scale is used for representing moderate risk. The hue values for other risk colourings were also slightly adjusted to produce a good distinction between different colours, the effect of which can be seen in Figure \ref{fig:stage1bennevis} (right).

\section{Combining the static risk model and the dynamic risk model}  \label{sec:combine-models}

As shown in Figure \ref{fig:eu-avalanche-scale}, the dynamic risk model produced by the SAIS has five distinct bands of risk levels (six if the grey data-not-available is included). In contrast, as shown in Section \ref{sec:static-factors}, the static risk model contains continuous data. This requires an appropriate method to combine the two models for visualisation purposes in the application. While it is possible to merge the two models numerically again through either a Support Vector Machine (SVM) or a fuzzy logic operator, due to the lack of an established precedence between the influence of the static model and the dynamic model used in this project, the use of different representations for the two models was instead preferred. 

As described in Section \ref{sec:colour-representation}, the two main schemes for colour representation of risk are saturation-based and hue-based. This provides an opportunity for implementing a mixed approach, in which two different representations can be used: one with hue and the other with saturation. 

In particular, the dynamic risk level will determine the value of hue at a point, thus determining the "colour" of the point in the traditional sense, which is in turn determined by its altitude and terrain aspect as intended by the SAIS. The static risk level will then determine the saturation at that point. This means that the less likely static risk analysis believes that an avalanche release could occur at a point, the fainter the colour of that point will be. Conversely, a strong colour implies a high likelihood of avalanche release at a point based on static risk analysis.

In practise, thresholding will be applied to all static risk levels in the raster for the scaling of static risk value to saturation. To reduce the influence of extreme values on the visualisation of static risk, the static risk values are scaled between the 1st percentile and the 99th percentile rather than between the minimum and maximum risk values.

The resulting risk colour scheme, used in \textbf{Stage 2}, can be seen in Figure \ref{fig:final-colour-scheme}. The effects of static risk controlling saturation can also be observed in Figure \ref{fig:stage1bennevis} (right). 

\begin{figure}[h]
		\centering
		\includegraphics[height=5cm]{final-colour-scheme.png}
		\caption{\label{fig:final-colour-scheme} The new risk colour scheme implemented in \textbf{Stage 2}. Each row represents a dynamic risk level.}
\end{figure}

\section{A* pathfinding in a risk-adjusted environment} \label{sec:risky-pathfinding}

As described in Section \ref{sec:terrain-pathfinding}, A* search is a well-studied algorithm for pathfinding in a graph, especially in challenging conditions such as large datasets and strict or real-time timing constraints. It has been chosen as the basis for the pathfinding feature in \textbf{Stage 3} over a simpler implementation of Dijkstra's Algorithm, due to the convenience of introducing avalanche risk into the former's heuristic function, as well as former's usually superior performance \cite{goldberg2005computing} when only an approximate answer is required.

There are nevertheless a number of challenges when adapting A* search for the specific purpose of risk-adjusted pathfinding in a 3D terrain, including calculating the vertical distance between two points, balancing costs in distance and risk as required, and devising a suitable heuristic function for the problem. These will be addressed in the next few subsections, followed by an overview of the adapted algorithm. 

\subsection{Vertical distance between two points}

As terrain rasters arrange data in a grid with uniform resolution, horizontal distances between neighbouring nodes are consistent. Therefore, the only distinction of distance between two neighbouring nodes is the vertical distance. While it is possible to calculate the linear distance between two points based on trigonometry, this is both computationally expensive for a large data grid and inaccurate: it is unlikely that the terrain surface is smooth and straight between two neighbouring points. As an alternative, it is possible to estimate this distance based on observations and average cases: the Naismith's Rule, as introduced in \ref{sec:terrain-pathfinding}.

The Naismith's Rule estimates the amount of equivalent distance walked by a mountaineer to ascend or descend each unit of vertical distance, called the Naismith distance. In average case on a natural terrain, it has been estimated that walking on a levelled terrain for 3 miles is equivalent to ascending or descending 2000 feet vertically, which is equivalent to 7.92 meters of walking distance for each meter of vertical distance, as summarised by Scarf \cite{scarf2008mathematical}. This allows a simple conversion process to be established as followed:

\begin{equation} \label{eq:naismith-equation}
\begin{aligned}
\fontsize{10}{11}\selectfont
Naismith\_Distance(NodeA,\ NodeB) &= Raster\_Resolution(NodeA,\ NodeB) \\
&+ 7.92 * | NodeA.height - NodeB.height | \\
\text{where Raster\_Resolution returns the }&\text{raster resolution in the direction of PointA} \\
\text{to PointB, which is either straight or }&\text{diagonal.} 
\end{aligned}
\end{equation}

\subsection{Balancing costs in distance and risk}

A requirement of the project requires the algorithm to accept a user-configurable weighting of risk against distance, allowing the user to receive several possible routes between two points based on the importance they give to the risk enroute and the overall distance. After the user has requested a path with such a risk weighting, the application needs to calculate the overall cost of moving between two nodes accordingly. The risk weighting is between 0 and 1, representing the percentage of overall cost attributed to avalanche risk.

While it is simple to multiply the separately calculated distance cost and avalanche risk cost by their defined weights, it is also important that the two costs are normalised into the same scale before combining them. This requires the maximum and minimum distance cost and avalanche risk cost to be calculated beforehand. The full process is shown as followed:

\begin{equation} \label{eq:risk-weighting-equation}
\begin{aligned}
\fontsize{10}{11}\selectfont
normalised\_distance &= \frac{Naismith\_Distance(NodeA,\ NodeB) - min\_distance}{max\_distance - min\_distance} \\
normalised\_risk &= \frac{NodeB.risk - min\_risk}{max\_risk - min\_risk} \\
overall\_movement\_cost &= normalised\_risk \cdot risk\_weighting \\
&+ normalised\_distance \cdot (1 - risk\_weighting)
\end{aligned}
\end{equation}

It is worth noting that the avalanche risk of moving between two nodes is dependent on the target node only, as calculating a potentially negative difference in risk levels between two nodes could affect the admissibility of the heuristic function.

\subsection{Heuristic function for risk-adjusted A* search}

The heuristic function used to determine the priority under which each node is explored considers not only the Naismith distance between the candidate node and the goal node, but also the immediate static risk of moving to the candidate node from the current node. The latter creates a risk-averse behaviour for the algorithm, which helps the pathfinder in avoiding localised avalanche hotspots.

Due to the fact that the search will be conducted on a rectangular section of the data grid whose position is determined by the location of the start node and the goal node, and that it is possible to move diagonally in the real world, the commonly used goal distance estimation based on the Manhattan Distance must be altered, as shown in Figure \ref{fig:heuristic-distance}. 

\begin{figure}[h]
		\centering
		\includegraphics[height=3cm]{heuristic.pdf}
		\fontsize{10}{11}\selectfont
		\begin{equation}\label{eq:heuristic-equation}
		\begin{aligned}
		estimated\_distance &=  (x - y) + \sqrt{2 \cdot y^2} \\
		&+ 7.92 * | NextNode.height - GoalNode.height | \\
		heuristic\_cost &= estimated\_distance \cdot NextNode.risk \\
		scaled\_heuristic &= \frac{heuristic\_cost - min\_naismith}{max\_naismith - min\_naismith}
		\end{aligned}
		\end{equation}
		\caption{\label{fig:heuristic-distance} Calculating the scaled heuristic cost, where \textit{x} represents the longer side of the rectangle.}
\end{figure}

The heuristic cost of moving to a node can then be calculated by multiplying the risk level of the next node with the estimated Naismith distance to the goal node. The resulting heuristic cost is then scaled to be between the maximum and minimum actual Naismith distances calculated. This combines with the actual cost of moving to the node, as shown in (\ref{eq:heuristic-equation}).

\subsection{Overview of the adapted A* algorithm}

\begin{algorithm}
\caption{\label{alg:a-star}Pseudocode for the adapted A* search algorithm.}
\begin{algorithmic}[1] 
\State $search\_queue\gets$ PriorityQueue()
\State $search\_queue$.push(priority $\gets$ 0, $initial\_node$)
\State $source\_index\gets$ \{\}
\State $costs\_dict\gets$ \{\}
\State $source\_index[initial\_node]\gets$ None
\State $costs\_dict[inital\_node]\gets$ 0
\While{$search\_queue \not=$ empty} 
    \State $current\_node \gets$ $search\_queue$.pop();
    \If{$current\_node = goal\_node$}
         \Comment{Goal found, terminate.}
        \State break
    \EndIf
    \State $neighbours\gets$ get\_neighbours($current\_node$)
    \ForAll{$neighbour\_node \In neighbours$}
        \State $cost\gets$ naismith\_distance($current\_node$, $neighbour\_node$) * (1 - $risk\_weighting$) + $neighbour\_node$.risk * $risk\_weighting$;
        \State $new\_cost\gets$ $costs\_dict[current\_node]$ + $cost$;
        \If{$neighbour\_node \NotIn costs\_dict \Or new\_cost < costs\_dict[neighbour\_node]$}
            \State $costs\_dict[neighbour\_node]\gets$ $new\_cost$ \Comment{A shorter path.}
            \State $neighbour\_priority\gets$ $new\_cost$ + heuristic($neighbour\_node$, $goal\_node$)
            \State $search\_queue$.push(priority $\gets$ $neighbour\_priority$, $neighbour\_node$)
            \State $source\_index[neighbour\_node]\gets$ $current\_node$
        \EndIf
    \EndFor
\EndWhile
\State reconstruct\_path($goal\_node$, $source\_index$);
\end{algorithmic}
\end{algorithm}

\begin{figure}[h]
		\centering
		\includegraphics[height=6cm]{a-star.pdf}
		\caption{\label{fig:a-star} The operations of the pathfinder implementation.}
\end{figure}

As shown in Figure \ref{fig:a-star}, a few pre-processing steps are required to ensure that the algorithm can finish quickly with a meaningful result. If the bounding box created by the initial and final node coordinates is too small (occurs when the line between two nodes is almost in parallel with a longitude or latitude line), the search area is enlarged to allow the algorithm more choices. If the search space is too large, linear down-sampling is applied, picking the maximum height and risk level in the sample (for conservative estimations of Naismith distance and risk), and the mean aspect angle in the sample. 

Thereafter, the Naismith distances from each node to its neighbours are calculated by applying (\ref{eq:naismith-equation}). A clipping of risk values to between the 5th percentile of non-zero risk values and the maximum risk is also conducted, as preliminary testing of the implementation showed that blocks of zero risk values significantly reduce the performance of the algorithm. 

Finally, the A* search algorithm is performed on the pre-processed search space, as shown in Algorithm \ref{alg:a-star}. A path in the form of a list of coordinates can then be constructed through backtracking the search space from the goal node.

\section{Statistical evaluation of forecast accuracy} \label{sec:stats-eval}

In order to determine the quality of the avalanche risk models used by the project, it is essential to conduct some statistical tests on the model. An ordinary approach is to conduct an inferential statistical test such as the Mann-Whitney U test \cite{mann1947test} to determine whether our models accurately determine the locations of actual avalanche releases. However, as there is no prior static risk model applied to the Scottish Highlands that we are aware of, this is not possible.

As described in Section \ref{sec:combine-models}, our static risk model is combined with the dynamic model through multiplication at runtime, as the dynamic factors changes too frequently to efficiently store a combined model. This presents a major difficulty in evaluating the combined model, in addition to historic SAIS forecasts being incomplete (compass rose only available since 2009 with large gaps), as well as the computational difficulties in storing 5-6 large dynamic risk rasters for each of the thousands of days. Therefore, our statistical evaluation will focus on the static risk model only, as described in Section \ref{sec:risk-model-eval}.

\chapter{Design and Implementation of the Application} \label{ch:app-description}

This chapter provides details on the implementation of the software system. Starting with an overview on the architecture design in Section \ref{sec:app-system}, different levels of the implementation will be covered: retrieval and processing of various types of data (\ref{sec:data-processing}), the API servers serving as an intermediary between data and the user interface (\ref{sec:api-server}), and the user interface itself (\ref{sec:user-interface}).

\section{Architecture of the system} \label{sec:app-system}

As shown in Figure \ref{fig:new-system}, the system is structured across four levels: Data Store, Data Interface, API Servers and Front-end. The first two are collectively described in detail in Section \ref{sec:data-processing}, and the last two in \ref{sec:api-server} and \ref{sec:user-interface} respectively. Each level abstracts away the implementation details of the previous level by providing interfaces of interaction for the next level, allowing maintenance and extensions to be conducted easily on the codebase.

\begin{figure}[h]
		\centering
		\includegraphics[height=8cm]{System-new.pdf}
		\caption{\label{fig:new-system} The architecture of the system. All components in Data Interface, API Servers and Front-end are implemented from scratch during this project, except the Terrain Server. During live operations, a user request is handed down each level from the Front-end to the Data Store, and returned in the reverse order, forming the operation flows.}
\end{figure}

The Front-end takes the form of a Cesium application running in the user's web browser, served by Nginx running on the application server. Nginx also terminates the TLS connection from the user to ensure secure and tamper-resistant data transmission. As the user interacts with the virtual 3D scene in Cesium, it will automatically request the necessary data from the API servers on the next level -- either overlay images through the Imagery API or route and past avalanche data through the Data API.

The intermediate level, API Servers, contains the two Python APIs implemented with the Flask library: Imagery API and Data API (covered in \ref{sec:api-server}). Upon receiving data requests, the APIs will first determine whether the requests are of valid form, and scrutinise the parameters. Once a request is verified, the handling API will make function calls to both the utility library and the modules of the next level -- Data Interface. In addition to serving the Front-end, Nginx also provides reverse proxy for the API Servers, passing requests and returned data in between them and Front-end.

Data Interface is the level where specialised functionalities are implemented, and modules interact with stored data. They are called upon by specific APIs for specific purposes, such as pathfinding or retrieving past avalanche records. For operational safety, input sanitisation is conducted again at this level. 

Data Store is the lowest level, where various types of data required for the operation are stored. Some types of data such as terrain models are stored as static files once computed, while others are stored in a SQLite database for dynamic update and retrieval.

\section{Data retrieval and processing} \label{sec:data-processing}

The Data Store of the application stores various form of data, some of which require pre-processing before usage (static risk data, \ref{subsec:static-risk-processing}), while others are fetched and stored directly live (\ref{subsec:dynamic-data-processing} and \ref{subsec:past-avalanche-data}. This section covers both their sourcing and pre-processing (if applicable). 

\subsection{Dynamic risk data} \label{subsec:dynamic-data-processing}

As described in Section \ref{sec:motivation}, the Scottish Avalanche Information Service (SAIS)\cite{sais} provides avalanche forecasts winter seasons. The most prominent scale of risk is the hazard compass rose \cite[p. 4]{sais-report}, as shown in Figure \ref{fig:compassrose}. 

		\begin{figure}[h]
		\centering
		\includegraphics[scale=0.3]{CompassRose.png}
		\caption{\label{fig:compassrose}An example compass rose produced by the SAIS.\cite[p. 4]{sais-report}}
		\end{figure}
		
Unfortunately, the SAIS does not provide an API with which the risk levels making up the compass rose could be retrieved easily. Therefore, it was necessary to build a web crawler (\textit{SAISCrawler}) with the \href{http://docs.seleniumhq.org/}{Selenium} library to automatically search for and download avalanche forecasts from the SAIS.

Vertically, the forecast data is divided by altitude into two sectors, each of which is further divided by terrain aspect into eight sections, each representing a 30\si{\degree} circular segment, as depicted in Figure \ref{fig:compassrose}. Each segment contains two risk levels: the primary level and the secondary level. Therefore, a total of 32 risk level values are extracted from each compass rose. 
		
In conformity with the entity-relationship model, eight forecast records are created for the eight circular segments, each containing the primary and secondary risk levels for the two altitude sectors in that segment, along with the date of forecast and the transition altitudes. These records are stored in a \href{https://sqlite.org/}{SQLite} database by the crawler. An example set of forecast records is depicted in Figure \ref{fig:sqlite}.
		
    		\begin{figure}[h]
    		\centering
    		\includegraphics[scale=0.6]{SQLite.pdf}
    		\includegraphics[scale=0.5]{DangerousRose.png}
    		\caption{\label{fig:sqlite}An example set of forecast records (left) extracted from a compass rose (right) \cite{sais}.}
    		\end{figure}
		
As a conservative assessment of dynamic risk, the higher of the primary and the secondary risk levels in each section is used as the dynamic risk of that section in processing.

\subsection{Altitude, aspect and static risk data} \label{subsec:static-risk-processing}

As static risk data originate from analyses of the terrain model, it is necessary to obtain a full Digital Elevation Model (DEM) for the areas of interest (Scottish Highlands). There are a number of available sources for DEM data of the United Kingdom, most notably the EU-DEM Data \cite{eu-dem} provided by the European Commission (25-30m resolution, open license), and the UK Government's Ordnance Survey Data (OS5) \cite{os-5} (5m resolution, limited license, mentioned in \ref{sec:modelling-avalanche}).
		
With considerations on the nature of the project and the potential uses of the application, the Ordnance Survey Data is used for the course of the project, as an elevation model with higher accuracy is beneficial for accurate computations and evaluations. However, should an expansion of coverage or usage be required, the EU-DEM Data can be processed and used with the same procedures.
		
The Ordnance Survey Data is then merged and reprojected by GDAL into the WGS84 geodetic system \cite{wgs84} that is used by the majority of mapping and navigation systems:

\begin{lstlisting}[breaklines]
find ./OS5/ -name '*.tif' | xargs gdal_merge.py -of GTiff -o OS5_Merged.tif 
gdal_translate -a_srs EPSG:4326 -of GTiff OS5_Merged.tif WGS84.tif
\end{lstlisting}
		
The reprojected elevation (height-map) raster {\ttfamily WGS84.tif} can now be used to look up altitude for each point in the geographical area covered by the DEM Data. However, we must also analyse the elevation raster to infer a few other sets of raster data: a raster storing terrain aspect at each point is required for both dynamic (\ref{sec:dynamic-factors}) and static risk analysis (\ref{subsec:roughness}); a raster storing terrain slope angle is part of the static risk model (\ref{subsec:slope}); as well as a raster storing terrain curvature (\ref{subsec:curvature}). These rasters are computed with the {\ttfamily Slope, Aspect, Curvature} tool in the \href{https://grass.osgeo.org/grass64/manuals/r.slope.aspect.html}{GRASS} library.

The final Static Risk raster storing the static risk level of each point in the risk model is one-time computed with a MATLAB script, using data from aspect, slope angle, and curvature rasters. The methodology behind this script has been covered in detail in Section \ref{sec:static-factors}.

For the convenience of the user, a map raster directly downloaded from The Ordnance Survey is also stored, providing overlay images of roads, location names and contours lines.

The terrain shape files used by Cesium to display 3D terrain models can also be built and served with third-party tools developed by the University of Southampton \footnote{cesium-terrain-builder: https://github.com/geo-data/cesium-terrain-builder}, which provides an alternative to the default terrain shape files served by the maintainers of the Cesium library.

\subsection{Past avalanches} \label{subsec:past-avalanche-data}

As reflected in the requirements, the ability to label the location of past avalanches on the map was requested. The SAIS provides an \href{www.sais.gov.uk/avalanche_map/}{Avalanche Map} which labels the time, approximate location and additional descriptions of each past avalanche in the monitored area, as reported by both the SAIS field forecasters and users of the website. This information is a good source of reference data for statistical evaluations of the risk model. However, it is worth noting that the existence, the date, and the location of each avalanche are approximate only.

A crawler similar to the one used for retrieving SAIS avalanche forecasts (\ref{subsec:dynamic-data-processing}) was written for retrieving past avalanche records. The data is stored in the format shown in Figure \ref{fig:sais-past-avalanche}.

\begin{figure}[h]
		\centering
		\fontsize{9}{11}\selectfont
		\begin{tabular}{ | c | c | c |}
  		\hline
		Field & Type & Description \\ \hline   
  		avalanche\_internal\_id & INT & Our ID of avalanche record, primary key. \\ \hline
		avalanche\_id & INT & The SAIS' ID of the avalanche for update checking. \\ \hline
		easting & INT & The horizontal coordinate value of the location in OSGB36 format. \\ \hline
		norting & INT & The vertical coordinate value of the location in OSGB36 format.  \\ \hline
		avalanche\_time & TEXT & The date and time of the recorded avalanche. \\ \hline
		avalanche\_comment & TEXT & Additional descriptions of the avalanche supplied by the SAIS. \\ \hline
		\end{tabular}
		\caption{\label{fig:sais-past-avalanche}The format of a stored past avalanche record, stored in SQLite.}
\end{figure}

\section{The freshly-implemented API servers} \label{sec:api-server}

Serving as the intermediary between the user and the data processors, the API Servers level provide two categories of APIs required for implementing the actions of the application: Data API and Imagery API, both of which will be covered in this section. 

\subsection{Imagery APIs} \label{subsec:imagery-api}

APIs returning an image on valid calls are categorised as Imagery APIs. This includes: {\ttfamily avalanche\_risks}, {\ttfamily terrain\_aspects}, and {\ttfamily contours}. The images returned are PNG images varying levels transparency for optimised interlacing.

\textbf{{\ttfamily avalanche\_risks}}: this is the main imagery API (Figure \ref{fig:avalanche-risks-doc}), responsible for returning per-point risk level images for either dynamic-only (\textbf{Stage 1}) or static-and-dynamic (\textbf{Stage 2}) risk models (\ref{sec:combine-models}). Its backend operations are shown in Figure \ref{fig:avalanche-risks-retrieval}.

\begin{figure}[h]
		\centering
		\includegraphics[scale=0.6]{Retrieval.pdf}
		\caption{\label{fig:avalanche-risks-retrieval}A diagram illustrating the retrieval of any raster's data with geodetic coordinates. In order to ensure the continuity of operations, APIs return empty content and silently log errors when an error is encountered, instead of breaking out.}
\end{figure}

\begin{figure}[h]
\begin{lstlisting}[breaklines]
/imagery/api/v1.0/avalanche_risks/<string:longitude_initial>/<string:latitude_initial>/<string:longitude_final>/<string:latitude_final>/<string:forecast_date>?showStaticRisk=[0,1]
\end{lstlisting}
{\small
Required: {\ttfamily longitude\_initial, latitude\_initial, longitude\_final, latitude\_final}.\\
Optional: {\ttfamily forecast\_date} (default: today's date), {\ttfamily showStaticRisk} (default: 0).
}
\caption{\label{fig:avalanche-risks-doc}The API call format for {\ttfamily avalanche\_risks}. The parameter format of coordinate components separated by slashes are designed specifically for Cesium's \textit{UrlTemplateImageryProvider} \cite{urltemplate} class to automatically retrieve risk level images as tiles.}
\end{figure}

\textbf{{\ttfamily terrain\_aspects}}: this API works in a similar way as {\ttfamily avalanche\_risks}, but instead of retrieving risk level images, it retrieve terrain aspect images encoded by the full spectrum of RGB colours. It was used for debugging aspect fitting and generating terrain facing maps.

\textbf{{\ttfamily contours}}: this also works in a similar way as {\ttfamily terrain\_aspects} and as shown in Figure \ref{fig:avalanche-risks-retrieval}. However, it returns general map images as described in Section \ref{subsec:static-risk-processing}, rather than risk level images.

The same call format for both {\ttfamily terrain\_aspects} and {\ttfamily contours} is shown in Figure \ref{fig:terrain-aspects-doc}.

\begin{figure}[h]
\begin{lstlisting}[breaklines]
/imagery/api/v1.0/[terrain_aspects,contours]/<string:longitude_initial>/<string:latitude_initial>/<string:longitude_final>/<string:latitude_final>
\end{lstlisting}
{\small Required: {\ttfamily longitude\_initial, latitude\_initial, longitude\_final, latitude\_final}}.
\caption{\label{fig:terrain-aspects-doc}The API call format for both {\ttfamily terrain\_aspects} and {\ttfamily contours}.}
\end{figure}

\subsection{Data APIs} \label{subsec:data-api}

APIs returning JSON datasets on valid calls are categorised as Data APIs, providing data for the UI (Section \ref{sec:user-interface}). This includes: {\ttfamily forecast\_dates}, {\ttfamily find\_path}, and {\ttfamily past\_avalanches}.

\textbf{{\ttfamily forecast\_dates}}: This simple API (Figure \ref{fig:forecast-dates-doc}) returns up to 50 most recent SAIS forecast dates of a location defined by a single geodetic point, supplying data for the Past Forecast Picker.

\begin{figure}[h]
\begin{lstlisting}[breaklines]
/data/api/v1.0/forecast_dates/<string:longitude>/<string:latitude>
\end{lstlisting}
{\small Required: {\ttfamily longitude, latitude}}.
\caption{\label{fig:forecast-dates-doc}The API call format for {\ttfamily forecast\_dates}.}
\end{figure}

\textbf{{\ttfamily find\_path}}: This API (Figure \ref{fig:find-path-doc}) interfaces the Path Finder module, which implements risk-adjusted pathfinding as required in \textbf{Stage 3}, and described in Section \ref{sec:risky-pathfinding}. 

\begin{figure}[h]
\begin{lstlisting}[breaklines]
/data/api/v1.0/find_path/<string:longitude_initial>/<string:latitude_initial>/<string:longitude_final>/<string:latitude_final>/<string:risk_weighing>/<string:forecast_date>
\end{lstlisting}
{\small Required: all parameters except {\ttfamily forecast\_date} (default: today's date).}
\caption{\label{fig:find-path-doc}The API call format for {\ttfamily find\_path}.}
\end{figure}

\textbf{{\ttfamily past\_avalanches}}: Another simple API (Figure \ref{fig:past-avalanches-doc}). Given a start and an end date, it returns a list of recorded avalanches between the dates, used by the Past Avalanche Range Picker. The returned data for each avalanche is identical to that of Figure \ref{fig:sais-past-avalanche}. 

\begin{figure}[h]
\begin{lstlisting}[breaklines]
/data/api/v1.0/past_avalanches/<string:start_date>/<string:end_date>
\end{lstlisting}
{\small Required: {\ttfamily start\_date, end\_date} (must be in ascending order).}
\caption{\label{fig:past-avalanches-doc}The API call format for {\ttfamily past\_avalanches}.}
\end{figure}

\section{The user interface} \label{sec:user-interface}

The user interface of the application is implemented with the Cesium WebGL library, which runs in most modern web browsers, as shown in Figure \ref{fig:ui}. The vast majority of the interface area are left to the virtual 3D scene that the user can navigate freely in, while the main features of the application are accessed via UI elements on the top-left corner of the interface.

\begin{figure}[h]
    		\centering
    		\includegraphics[scale=0.5]{UI.pdf}
    		\caption{\label{fig:ui}The user interface of the final product. The red-dots indicate the locations of past avalanches.}
\end{figure}

From top to bottom, there are five elements for feature access: Terrain Picker (dropdown for debugging), Past Forecast Picker (dropdown), Past Avalanche Range Picker (dropdown), Risk Model Switcher (button), and Pathfinder (panel).

The interface is programmed in JavaScript, interfacing the APIs provided by Cesium. Extensive use of asynchronous XHTML requests allows seemingless transmission of dropdown options, user inputs and returned data, as well as necessary interface changes and map labelings required by the returned data.

The user interface is designed in conformance with the principles described in Section \ref{sec:application-usability}. While it is necessary to use a mouse or a touchscreen to interact with the application, the mouse proficiency required on the user \cite{Dubois:2007:EAI:1531407.1531416} is no greater than what's required by a standard map application such as Google Maps, as no new types of interactions are programmed. The Cesium library also implements the style of touchscreen interactions recommended by \cite{ku2014study}. The application also responded well to rough touchscreen inputs \cite{Albinsson:2003:HPT:642611.642631} during test uses.

\chapter{Evaluation and Testing of the Application} \label{ch:app-testing}

The evaluations conducted on the application attempt two answer two questions: how usable is the application to the user; and how accurate our devised risk model is. Section \ref{sec:usability-eval} attempts to answer the first question, while Section \ref{sec:risk-model-eval} attempts to answer the second.

\section{Evaluation and testing of usability} \label{sec:usability-eval}

In usability testing, another experienced mountaineer was invited to use both the traditional map-and-forecast method and the new application to plan routes in the Lochaber area, as if climbing on a day with recorded avalanches. Without knowledge of the recorded avalanches, the user successfully avoided planning a route through the avalanched area with both methods. The user's comments and any associated changes we made are shown in Figure \ref{fig:user-testing-table}.

\begin{figure}[h]
		\centering
		\includegraphics[scale=0.15]{user-experimentation.jpg}
		\caption{Rob Stone, an experienced mountaineer, testing the traditional map-and-forecast method before test-using the application of this project.}
\end{figure}

\begin{figure}[htbp]
\centering 
\fontsize{9}{12}\selectfont
\begin{tabular}{ | p{8cm}| p{8cm}|}
\hline
\textbf{User Feedback} & \textbf{Our Action (if applicable)} \\ \hline
\multicolumn{2}{|P{\linewidth}|}{\textbf{Data and Data Models}} \\ \hline
More dynamic data layers will be useful, such as snowfall levels and weather. & Data will have to be specially sourced and regularly updated. For future work due to insufficient time. \\ \hline
Risks of cornice avalanches near ridges are not sufficiently considered. & Can be taken into account in the static risk model,  difficulties in determining ridges require future work. \\ \hline
Use of crowdsourcing avalanche records, and risk area labelings by experts could help enhance the risk model. & This is possible if this application can be commercialised. \\ \hline
The data model does not consider vertical effects of risk, e.g. a high release risk area would increase the risk of areas below it. & With a decay-by-distance function, this should be easily computable, but requires adjustment of the decay parameters with repeated computations, which is beyond our current computational capabilities. \\ \hline
\multicolumn{2}{|P{\linewidth}|}{\textbf{The Pathfinder}} \\ \hline
Setting the weighting factor to be 100\% minimising risk would cause impractical routes to be returned. & The limit on the weighting factor has been adjusted to a maximum of 90\% risk in the UI. \\ \hline
Multiple subsequent routes returned should have different colours or labels for easy identification. & This has been implemented, subsequent routes will loop between six possible colours. \\ \hline
Walking on the ridges are usually the least risky routes, but the pathfinder's behaviour is not consistent in this. & Adjustments to the heuristic function and Naismith's Rule factor require significant future work. \\ \hline
\multicolumn{2}{|p{\linewidth}|}{The Pathfinder may not be an appropriate feature, as any inappropriate route returned by the pathfinder would affect the user's confidence in the entire application. Users should probably be encouraged to work out the route based on the risk map of the application and their own experience. However, this functionality can be a very useful training tool.} \\ \hline
\end{tabular}
\caption{\label{fig:user-testing-table}Comments and suggestions made by the potential user during usability testing, and associated actions we took to improve the application.} 
\end{figure}
\section{Evaluation of the risk model} \label{sec:risk-model-eval}

In order to evaluate the accuracy of the static risk model as described in Section \ref{sec:static-factors}, a Python script was produced to read in data from modules in Data Interface, and compare the risk model with past avalanche records (nearly 4,000 since 1991) to measure the accuracy of the model.

As our risk model essentially computes the probability of each point being an avalanche release area, a common evaluation strategy for release area estimation models can be used: calculate the accuracy of release area identification at different percentages of areas of study, which produces two important metrics: the accuracy itself, and the percentage of area identified as at risk -- if this is too large then the accuracy becomes meaningless.  Since our risk model produces a continuous value for each point, risk thresholds can be set at different percentiles of all risk values to create definite percentages of identified areas.

Unfortunately, as discussed in Section \ref{subsec:past-avalanche-data}, the avalanche locations pin-pointed by the SAIS's Avalanche Map are far from exact. Therefore, it was necessary for us to search in a square box around the pined location of each avalanche, with varying extents (starting from 25m).

A histogram of all risk values is first produced to help determine the risk thresholds, as shown in Figure \ref{fig:risk-eval-hist}. Based on the distribution of risk values, the thresholds were selected as each whole number between 70\% and 99\%, as well as 99.5\% and 99.9\% to test the nature of extreme risk values. The value of each threshold is calculated, and a point is identified as a potential release area under that threshold if its value is above the threshold's risk value. The results of this evaluation is shown in Figure \ref{fig:stat-eval-table} and plotted in Figure \ref{fig:stat-eval-plot}.

\begin{figure}[h]
		\centering
		\includegraphics[scale=0.55]{static-risk-distribution.png}
		\caption{\label{fig:risk-eval-hist}A histogram showing the distribution of risk values in the static risk model (1991 to March 16, 2017). The main histogram is capped at a count of 1,000,000 due to the distinct values of high risk points, as shown in the overview diagram at top-right. The total number of points in the full raster is just under 1,000,000,000.}
\end{figure}

\begin{figure}[h]
\centering 
\fontsize{9}{11}\selectfont
\begin{tabular}{ | c| c|| >{\bfseries}c| c| c| c| }
\hline
Threshold \% & \% of study area & Search 25m & Search 50m & Search 125m & Search 250m \\ \hline
70\% & 30\% & 98.69\% & 98.77\% & 98.79\% & 98.79\%  \\ \hline
80\% & 20\% & 97.92\% & 98.67\% & 98.79\% & 98.79\%  \\ \hline
90\% & 10\% & 96.20\% & 98.28\% & 98.72\% & 98.79\%  \\ \hline
95\% & 5\% & 93.48\% & 97.07\% & 98.56\% & 98.77\%  \\ \hline
99.5\% & 0.5\% & 70.12\% & 88.17\% & 96.48\% & 98.28\%  \\ \hline
99.9\% & 0.1\% & 27.41\% & 54.44\% & 82.60\% & 94.56\%  \\ \hline
\end{tabular}
\caption{\label{fig:stat-eval-table}The identification rate of our model's release area estimation at varying search area extents. A distinct trade-off between identification rate and false positive rate can be observed: the lower the risk value threshold, the higher the identification rate, but also consequently higher the false-positive rate. Therefore high identification rates at lower risk thresholds are not necessarily ``better'', but a balancing threshold needs to be chosen. This diagram presents a subset of values plotted in Figure \ref{fig:stat-eval-plot}.}
\end{figure}

\begin{figure}[h]
		\centering
		\includegraphics[scale=0.6]{static-risk-eval-plot.png}
		\caption{\label{fig:stat-eval-plot}A diagram showing the accuracy of our model's release area estimation at different risk threshold percentiles. Note that risk threshold = 100\% - \% of study area.}
\end{figure}

Before comparing the accuracy of our model against the performance of release area estimation of other models, it is important to note that percentages values cannot be compared directly due to the difference in size of study areas. Similar to the previous study in the Scottish Highlands by Purves \textit{et al.} \cite{Purves2003343}, our study area is significantly larger than many others, and the lack of precisely recorded avalanches added another layer of complications. Within an extent of 25 meters, covering 0.5\% of study area, our model produces a comparable performance (70.12\%) to others discussed in Section \ref{sec:modelling-avalanche}, as shown in Figure \ref{fig:eval-compare}.

\begin{figure}[h]
\centering 
\fontsize{9}{11}\selectfont
\begin{tabular}{ | p{2.2cm}| p{3.5cm} | p{2cm}| p{2cm}| p{2.5cm}| p{2cm}|}
\hline
Study & Method & Location & Study area & \% area identified as risky & Identification accuracy \\ \hline
Ghinoi and Chung \cite{Ghinoi2005305} & slope, aspect, elevation, crest distance, roughness and curvature & Italian Dolomites & $\approx 230 km^2$ & 7\%-20\% & 67-82\% \\ \hline 
Bühler \textit{et al.} \cite{buhler2013automated} & slope, curvature, roughness, land cover & Manali, India & $126 km^2$ & 36\% & 66.7\% \\ \hline 
Veitinger \textit{et al.} \cite{veitinger2016potential} & slope, wind shelter, roughness & Zuoz, Switzerland & $\approx 50 km^2$ & 20\% & 60-80\% \\ \hline
Pistocchi and Notarnicola \cite{Pistocchi2013} & statistical methods and morphology & Italian Alps & $7,400 km^2$ & 20\% & $\approx70\%$ \\ \hline
Purves \textit{et al.} \cite{Purves2003343} & statistical methods and dynamic observations & Scottish Highlands & $> 5,000 km^2$ & Not a static risk model, no localisation. & 71\%-82\% (broad area only) \\ \hline
Our Model & slope, aspect, curvature, roughness and dynamic observations & Scottish Highlands & $> 5,000 km^2$ & 0.5-30\% & 70.12\%-98.69\% (details in Figure \ref{fig:stat-eval-table}) \\ \hline 
\end{tabular}
\caption{\label{fig:eval-compare}Comparing the accuracy of our model against other release area estimation models, picking the most representative accuracy covering all avalanche releases in other studies. In most cases, the identification accuracy increases as the percentage of area allowed to be identified as risky increases.} 
\end{figure}

Despite the difficulties in comparing the identification accuracy of our model against other models directly, for the 0.5\% of area of Scottish Highlands identified as most likely avalanche release areas by our model, the positive rate searching within 25 meters is above 70\%. This increases to above 90\% for the top 5\% of areas.

As described in Section \ref{sec:combine-models}, the saturation of risk colour codings are determined by normalising the risk levels to between the 1st and 99th percentiles of all risk values. Therefore we believe that as long as the proportion of high risk points remain small (Figure \ref{fig:risk-eval-hist}), the application will be effective at visualising high avalanche risk spots in the Scottish mountains, such as shown in Figure \ref{fig:visual-confirmation}.

\begin{figure}[h]
		\centering
		\includegraphics[scale=0.95]{effective.pdf}
		\caption{\label{fig:visual-confirmation}Combined risk map of two areas in Ben Nevis, Lochaber, overlaid with past avalanche records since 1991. }
\end{figure}

\chapter{Conclusions} \label{ch:conclusions}

This chapter summarises what has been achieved by the project over its course, as well as potentials for related work to be conducted in the future.

\section{Summary of achievements}

This project implemented a new visualisation approach for avalanche hazards, providing a potential solution to the difficulties mountaineers face in correlating avalanche risk forecasts with mountainous terrains. The application follows usability considerations related to its potential uses.

While the dynamic risk model was based on data from the SAIS, a new static risk model was devised and evaluated, as well as methods of combining them in both visualisation and pathfinding. In addition to adaptations of existing topographical analysis techniques, new modelling techniques were devised together with a new risk colour coding. Evaluations demonstrated that our static risk model has better or comparable performance compared to past efforts \cite{buhler2013automated}\cite{Purves2003343}\cite{Pistocchi2013}\cite{veitinger2016potential}\cite{Ghinoi2005305} (Figure \ref{fig:eval-compare}), and when combined with the dynamic risk model effectively and accurately visualises avalanche hazards.

In the process, we also demonstrated that topographical analysis techniques used for avalanche forecasting in other regions can also be used in the Scottish Highlands with suitable adaptions such as what we made.

The A* search algorithm has been adapted by this project for mountain terrains in a risk-adjusted environment, providing a method for safety-critical pathfinding in areas with avalanche risk.

In statistical evaluation of the risk model, a threshold-based method for evaluating the risk model against data with poor localisation and coverage was created, which can be adapted for evaluating similar models in lack of reliable test data.

The application was developed with Git version control, and in accordance with software engineering principles. The codebase is well-structured, documented and can be easily redeployed on most Linux environments. It will be made publicly available for future work.

\section{Potential future work}

Based on user feedback during usability testing, various image overlays useful to mountaineers can be added with appropriate data sources, such as localised weather and snowfall records. There are also potential additions to the risk model, including the vertical effects of high risk areas, and the risk of cornice avalanches.

As mentioned in Section \ref{sec:risk-model-eval}, the extreme computational requirement of storing thousands of combined risk models has made evaluating a combined static and dynamic risk model difficult. With greater access to computational resources, our risk model can be further evaluated. 

During test uses, it was found that transferring and displaying large amount of data have slowed down the application, especially on devices with less computing power and slower network connections. In order to improve the usability of the application in the field, future developments could be conducted to migrate the user-end to native code and store pre-downloaded data to improve performance.

\small{\bibliography{project}}
\end{document}  